{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1446c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\n",
      "‚úÖ Src path added: c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\src\n",
      "üìÅ Carpetas procesadas detectadas:\n",
      "['c:\\\\Users\\\\Sofia\\\\RAGModel_MineriaMultimedia_202520\\\\data\\\\preprocessed\\\\processed_400_100',\n",
      " 'c:\\\\Users\\\\Sofia\\\\RAGModel_MineriaMultimedia_202520\\\\data\\\\preprocessed\\\\processed_600_150',\n",
      " 'c:\\\\Users\\\\Sofia\\\\RAGModel_MineriaMultimedia_202520\\\\data\\\\preprocessed\\\\processed_800_200']\n",
      "üìÑ Le√≠dos 28 registros desde c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\data\\preprocessed\\processed_400_100\n",
      "üìÑ Le√≠dos 21 registros desde c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\data\\preprocessed\\processed_600_150\n",
      "üìÑ Le√≠dos 17 registros desde c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\data\\preprocessed\\processed_800_200\n",
      "\n",
      "‚úÖ Total chunks cargados: 66\n",
      "üìä Ejemplo de formatted_chunk_id:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formatted_chunk_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_summary_400_100_chunk_01</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_summary_400_100_chunk_02</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_summary_400_100_chunk_03</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>3</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_01</td>\n",
       "      <td>breakingdawn_bookone</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_02</td>\n",
       "      <td>breakingdawn_bookone</td>\n",
       "      <td>2</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breakingdawn_bookthree_400_100_chunk_01</td>\n",
       "      <td>breakingdawn_bookthree</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        formatted_chunk_id               book_name  \\\n",
       "0            data_summary_400_100_chunk_01            data_summary   \n",
       "1            data_summary_400_100_chunk_02            data_summary   \n",
       "2            data_summary_400_100_chunk_03            data_summary   \n",
       "3    breakingdawn_bookone_400_100_chunk_01    breakingdawn_bookone   \n",
       "4    breakingdawn_bookone_400_100_chunk_02    breakingdawn_bookone   \n",
       "5  breakingdawn_bookthree_400_100_chunk_01  breakingdawn_bookthree   \n",
       "\n",
       "   chunk_number  word_count  \n",
       "0             1         400  \n",
       "1             2         400  \n",
       "2             3         253  \n",
       "3             1         400  \n",
       "4             2         186  \n",
       "5             1         400  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Conteo por chunk_size:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chunk_size\n",
       "400    28\n",
       "600    21\n",
       "800    17\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Top libros por n√∫mero de chunks:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "book_name\n",
       "newmoon                   16\n",
       "breakingdawn_bookthree    12\n",
       "breakingdawn_booktwo       9\n",
       "data_summary               7\n",
       "twilight                   7\n",
       "eclipse                    7\n",
       "breakingdawn_bookone       5\n",
       "midnightsun                3\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 689b1d7a-b644-44cc-a683-f8f30c565100)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Generando representaciones TF-IDF de los chunks...\n",
      "üìà Indexados 66 chunks.\n",
      "\n",
      "‚úÖ √çndice TF-IDF creado con 66 documentos\n",
      "üìä Dimensionalidad: 1538 features\n",
      "ü§ñ Cargando modelo de embeddings: sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
      "\n",
      "üìä Informaci√≥n de tokenizaci√≥n:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formatted_chunk_id</th>\n",
       "      <th>word_count</th>\n",
       "      <th>token_count_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_summary_400_100_chunk_01</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_summary_400_100_chunk_02</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_summary_400_100_chunk_03</td>\n",
       "      <td>253</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_01</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_02</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breakingdawn_bookthree_400_100_chunk_01</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        formatted_chunk_id  word_count  token_count_sample\n",
       "0            data_summary_400_100_chunk_01         400                   3\n",
       "1            data_summary_400_100_chunk_02         400                   3\n",
       "2            data_summary_400_100_chunk_03         253                   3\n",
       "3    breakingdawn_bookone_400_100_chunk_01         400                   3\n",
       "4    breakingdawn_bookone_400_100_chunk_02         186                   3\n",
       "5  breakingdawn_bookthree_400_100_chunk_01         400                   3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preparados 66 documentos para indexaci√≥n\n"
     ]
    }
   ],
   "source": [
    "# 02 - Indexaci√≥n Unificada\n",
    "\n",
    "# Objetivo: Crear √≠ndices TF-IDF, ChromaDB y FAISS a partir de chunks preprocesados.\n",
    "\n",
    "# Flujo del notebook:\n",
    "# 1. Cargar chunks desde data/preprocessed/processed_*\n",
    "# 2. Crear √≠ndice TF-IDF (baseline)\n",
    "# 3. Generar embeddings y poblar ChromaDB\n",
    "# 4. Construir √≠ndice FAISS\n",
    "# 5. Realizar consultas de prueba\n",
    "# 6. Comparar resultados entre m√©todos\n",
    "\n",
    "# ===================================\n",
    "# 1. Configuraci√≥n y Setup\n",
    "# ===================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configurar rutas del proyecto\n",
    "project_root = os.path.abspath(\"..\")\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "print(\"‚úÖ Project root:\", project_root)\n",
    "print(\"‚úÖ Src path added:\", src_path)\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "from utils import load_chunks_from_folder\n",
    "\n",
    "BASE_PREPROCESSED = os.path.join(project_root, \"data\", \"preprocessed\")\n",
    "folders = sorted([\n",
    "    os.path.join(BASE_PREPROCESSED, f) \n",
    "    for f in os.listdir(BASE_PREPROCESSED) \n",
    "    if f.startswith(\"processed_\")\n",
    "])\n",
    "\n",
    "print(\"üìÅ Carpetas procesadas detectadas:\")\n",
    "pprint.pprint(folders)\n",
    "\n",
    "# ===================================\n",
    "# 2. Carga de Datos y Metadata\n",
    "# ===================================\n",
    "\n",
    "records = []\n",
    "for folder in folders:\n",
    "    recs = load_chunks_from_folder(folder)\n",
    "    print(f\"üìÑ Le√≠dos {len(recs)} registros desde {folder}\")\n",
    "    records.extend(recs)\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "print(f\"\\n‚úÖ Total chunks cargados: {len(df)}\")\n",
    "df.head()\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "def make_formatted_id(row):\n",
    "    return f\"{row['book_name']}_{row['chunk_size']}_{row['overlap']}_chunk_{int(row['chunk_number']):02d}\"\n",
    "\n",
    "df['formatted_chunk_id'] = df.apply(make_formatted_id, axis=1)\n",
    "\n",
    "print(\"üìä Ejemplo de formatted_chunk_id:\")\n",
    "display(df[['formatted_chunk_id', 'book_name', 'chunk_number', 'word_count']].head(6))\n",
    "\n",
    "print(\"\\nüìà Conteo por chunk_size:\")\n",
    "display(df['chunk_size'].value_counts())\n",
    "\n",
    "print(\"\\nüìö Top libros por n√∫mero de chunks:\")\n",
    "display(df['book_name'].value_counts())\n",
    "\n",
    "# ===================================\n",
    "# 3. Indexaci√≥n TF-IDF (Baseline)\n",
    "# ===================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_tfidf_index(chunks):\n",
    "    if not chunks:\n",
    "        raise ValueError(\"‚ùå La lista de chunks est√° vac√≠a. No se puede crear el √≠ndice TF-IDF.\")\n",
    "    \n",
    "    print(\"‚öôÔ∏è Generando representaciones TF-IDF de los chunks...\")\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform(chunks)\n",
    "    print(f\"üìà Indexados {len(chunks)} chunks.\")\n",
    "    return vectorizer, X\n",
    "\n",
    "def query_tfidf(query, vectorizer, X, chunks, top_k=3):\n",
    "    if not query.strip():\n",
    "        raise ValueError(\"‚ùå La consulta est√° vac√≠a.\")\n",
    "    \n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    top_chunks = [(chunks[i], similarities[i]) for i in top_indices]\n",
    "    return top_chunks\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "# IMPORTANTE: Guardar documentos para TF-IDF antes de cualquier reasignaci√≥n\n",
    "documents_tfidf = df['text'].astype(str).tolist()\n",
    "vectorizer_tfidf, X_tfidf = create_tfidf_index(documents_tfidf)\n",
    "\n",
    "print(f\"\\n‚úÖ √çndice TF-IDF creado con {X_tfidf.shape[0]} documentos\")\n",
    "print(f\"üìä Dimensionalidad: {X_tfidf.shape[1]} features\")\n",
    "\n",
    "# ===================================\n",
    "# 4. Indexaci√≥n Vectorial (ChromaDB + FAISS)\n",
    "# ===================================\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "print(f\"ü§ñ Cargando modelo de embeddings: {embed_model_name}\")\n",
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "def token_count(text):\n",
    "    tokens = embed_model.tokenize(text)\n",
    "    return tokens['input_ids'].shape[1]\n",
    "\n",
    "df['token_count_sample'] = df['text'].apply(\n",
    "    lambda x: token_count(x) if len(x.split()) < 1000 else None\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Informaci√≥n de tokenizaci√≥n:\")\n",
    "display(df[['formatted_chunk_id', 'word_count', 'token_count_sample']].head(6))\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "ids = df['formatted_chunk_id'].astype(str).tolist()\n",
    "documents = df['text'].astype(str).tolist()\n",
    "metadatas = df.apply(lambda r: {\n",
    "    \"book_name\": r['book_name'],\n",
    "    \"chunk_size\": int(r['chunk_size']) if pd.notnull(r['chunk_size']) else None,\n",
    "    \"overlap\": int(r['overlap']) if pd.notnull(r['overlap']) else None,\n",
    "    \"chunk_number\": int(r['chunk_number']),\n",
    "    \"word_count\": int(r['word_count'])\n",
    "}, axis=1).tolist()\n",
    "\n",
    "print(f\"‚úÖ Preparados {len(ids)} documentos para indexaci√≥n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_venv)",
   "language": "python",
   "name": "rag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
