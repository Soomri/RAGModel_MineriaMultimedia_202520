{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495e19f3",
   "metadata": {},
   "source": [
    "**Resumen del flujo**\n",
    "1. Cargar todos los `_chunks.txt` de `data/preprocessed/processed_*`  \n",
    "2. Extraer metadata y construir `formatted_chunk_id` (ej: `twilight_400_100_chunk_3`)  \n",
    "3. Crear DataFrame de control y verificar distribuci√≥n de chunks/tokens  \n",
    "4. Construir embeddings y poblar ChromaDB  \n",
    "5. Ejecutar consultas de prueba y validar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc18b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\n",
      "Src path added: c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\src\n"
     ]
    }
   ],
   "source": [
    "# 1.1 ‚Äî Importaciones y configuraci√≥n de rutas\n",
    "import os, sys, pprint\n",
    "project_root = os.path.abspath(\"..\") \n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Src path added:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fb350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpetas procesadas detectadas:\n",
      "['c:\\\\Users\\\\Sofia\\\\RAGModel_MineriaMultimedia_202520\\\\data\\\\preprocessed\\\\processed_400_100',\n",
      " 'c:\\\\Users\\\\Sofia\\\\RAGModel_MineriaMultimedia_202520\\\\data\\\\preprocessed\\\\processed_600_150',\n",
      " 'c:\\\\Users\\\\Sofia\\\\RAGModel_MineriaMultimedia_202520\\\\data\\\\preprocessed\\\\processed_800_200']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import load_chunks_from_folder\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Folder base de preprocesados\n",
    "BASE_PREPROCESSED = os.path.join(project_root, \"data\", \"preprocessed\")\n",
    "\n",
    "# Mostramos las carpetas detectadas (sanity check)\n",
    "folders = sorted([os.path.join(BASE_PREPROCESSED, f) for f in os.listdir(BASE_PREPROCESSED) if f.startswith(\"processed_\")])\n",
    "print(\"Carpetas procesadas detectadas:\")\n",
    "pprint.pprint(folders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae9501",
   "metadata": {},
   "source": [
    "### 3 ‚Äî Cargar todos los `_chunks.txt` y crear un DataFrame unificado\n",
    "La funci√≥n `load_chunks_from_folder` devuelve una lista de diccionarios con campos:\n",
    "- chunk_id (num√©rico seg√∫n utils.py v1)\n",
    "- text\n",
    "- book_name\n",
    "- chunk_size\n",
    "- overlap\n",
    "- chunk_number\n",
    "- word_count\n",
    "\n",
    "En esta celda combinaremos todo y construiremos un `formatted_chunk_id` con el esquema:\n",
    "`{book_name}_{chunk_size}_{overlap}_chunk_{chunk_number:02d}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a831c678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le√≠dos 28 registros desde c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\data\\preprocessed\\processed_400_100\n",
      "Le√≠dos 21 registros desde c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\data\\preprocessed\\processed_600_150\n",
      "Le√≠dos 17 registros desde c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\data\\preprocessed\\processed_800_200\n",
      "Total chunks cargados: 66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>book_name</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>overlap</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1 Bella Swan moves from Phoenix, Arizona to th...</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Meanwhile, Victoria, a vengeful vampire, build...</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>their sparkling skin. 21 Charlie Swan, Bella‚Äôs...</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>After Bella's pickup truck dies a \"natural dea...</td>\n",
       "      <td>breakingdawn_bookone</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>love cautiously and Edward does not inflict an...</td>\n",
       "      <td>breakingdawn_bookone</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                               text  \\\n",
       "0         1  1 Bella Swan moves from Phoenix, Arizona to th...   \n",
       "1         2  Meanwhile, Victoria, a vengeful vampire, build...   \n",
       "2         3  their sparkling skin. 21 Charlie Swan, Bella‚Äôs...   \n",
       "3         4  After Bella's pickup truck dies a \"natural dea...   \n",
       "4         5  love cautiously and Edward does not inflict an...   \n",
       "\n",
       "              book_name  chunk_size  overlap  chunk_number  word_count  \n",
       "0          data_summary         400      100             1         400  \n",
       "1          data_summary         400      100             2         400  \n",
       "2          data_summary         400      100             3         253  \n",
       "3  breakingdawn_bookone         400      100             1         400  \n",
       "4  breakingdawn_bookone         400      100             2         186  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar datos\n",
    "records = []\n",
    "for folder in folders:\n",
    "    recs = load_chunks_from_folder(folder)\n",
    "    print(f\"Le√≠dos {len(recs)} registros desde {folder}\")\n",
    "    records.extend(recs)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df = pd.DataFrame.from_records(records)\n",
    "print(\"Total chunks cargados:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841601e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de formatted_chunk_id:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formatted_chunk_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_summary_400_100_chunk_01</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_summary_400_100_chunk_02</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_summary_400_100_chunk_03</td>\n",
       "      <td>data_summary</td>\n",
       "      <td>3</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_01</td>\n",
       "      <td>breakingdawn_bookone</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_02</td>\n",
       "      <td>breakingdawn_bookone</td>\n",
       "      <td>2</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breakingdawn_bookthree_400_100_chunk_01</td>\n",
       "      <td>breakingdawn_bookthree</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        formatted_chunk_id               book_name  \\\n",
       "0            data_summary_400_100_chunk_01            data_summary   \n",
       "1            data_summary_400_100_chunk_02            data_summary   \n",
       "2            data_summary_400_100_chunk_03            data_summary   \n",
       "3    breakingdawn_bookone_400_100_chunk_01    breakingdawn_bookone   \n",
       "4    breakingdawn_bookone_400_100_chunk_02    breakingdawn_bookone   \n",
       "5  breakingdawn_bookthree_400_100_chunk_01  breakingdawn_bookthree   \n",
       "\n",
       "   chunk_number  word_count  \n",
       "0             1         400  \n",
       "1             2         400  \n",
       "2             3         253  \n",
       "3             1         400  \n",
       "4             2         186  \n",
       "5             1         400  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteo por chunk_size:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chunk_size\n",
       "400    28\n",
       "600    21\n",
       "800    17\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 libros por n√∫mero de chunks:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "book_name\n",
       "newmoon                   16\n",
       "breakingdawn_bookthree    12\n",
       "breakingdawn_booktwo       9\n",
       "data_summary               7\n",
       "twilight                   7\n",
       "eclipse                    7\n",
       "breakingdawn_bookone       5\n",
       "midnightsun                3\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear formatted_chunk_id\n",
    "def make_formatted_id(row):\n",
    "    return f\"{row['book_name']}_{row['chunk_size']}_{row['overlap']}_chunk_{int(row['chunk_number']):02d}\"\n",
    "\n",
    "df['formatted_chunk_id'] = df.apply(make_formatted_id, axis=1)\n",
    "\n",
    "# Mostrar distribuci√≥n b√°sica\n",
    "print(\"Ejemplo de formatted_chunk_id:\")\n",
    "display(df[['formatted_chunk_id', 'book_name', 'chunk_number', 'word_count']].head(6))\n",
    "\n",
    "print(\"\\nConteo por chunk_size:\")\n",
    "display(df['chunk_size'].value_counts())\n",
    "\n",
    "print(\"\\nTop 5 libros por n√∫mero de chunks:\")\n",
    "display(df['book_name'].value_counts().head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b41d6",
   "metadata": {},
   "source": [
    "### 3 ‚Äî Cargar modelo de embeddings (SentenceTransformers: all-MiniLM-L6-v2)\n",
    "Este modelo es r√°pido y produce embeddings peque√±os y pr√°cticos para pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10ae513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo de embeddings: sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formatted_chunk_id</th>\n",
       "      <th>word_count</th>\n",
       "      <th>token_count_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_summary_400_100_chunk_01</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_summary_400_100_chunk_02</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_summary_400_100_chunk_03</td>\n",
       "      <td>253</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_01</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>breakingdawn_bookone_400_100_chunk_02</td>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breakingdawn_bookthree_400_100_chunk_01</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        formatted_chunk_id  word_count  token_count_sample\n",
       "0            data_summary_400_100_chunk_01         400                   3\n",
       "1            data_summary_400_100_chunk_02         400                   3\n",
       "2            data_summary_400_100_chunk_03         253                   3\n",
       "3    breakingdawn_bookone_400_100_chunk_01         400                   3\n",
       "4    breakingdawn_bookone_400_100_chunk_02         186                   3\n",
       "5  breakingdawn_bookthree_400_100_chunk_01         400                   3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "print(\"Cargando modelo de embeddings:\", embed_model_name)\n",
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "# Token count (subword) -> utilidad informativa\n",
    "def token_count(text):\n",
    "    tokens = embed_model.tokenize(text)\n",
    "    return tokens['input_ids'].shape[1]\n",
    "\n",
    "# Calculamos token_count en una muestra\n",
    "df['token_count_sample'] = df['text'].apply(lambda x: token_count(x) if len(x.split()) < 1000 else None)\n",
    "\n",
    "display(df[['formatted_chunk_id','word_count','token_count_sample']].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413d03f",
   "metadata": {},
   "source": [
    "### 5 ‚Äî Preparar listas para insertar en ChromaDB\n",
    "ChromaDB recibir√°:\n",
    "- ids: lista de `formatted_chunk_id`\n",
    "- documents: lista de textos (`text`)\n",
    "- metadatas: lista de dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea5b303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos a indexar: 66\n"
     ]
    }
   ],
   "source": [
    "# Preparar listas para Chroma / FAISS\n",
    "ids = df['formatted_chunk_id'].astype(str).tolist()\n",
    "documents = df['text'].astype(str).tolist()\n",
    "metadatas = df.apply(lambda r: {\n",
    "    \"book_name\": r['book_name'],\n",
    "    \"chunk_size\": int(r['chunk_size']) if pd.notnull(r['chunk_size']) else None,\n",
    "    \"overlap\": int(r['overlap']) if pd.notnull(r['overlap']) else None,\n",
    "    \"chunk_number\": int(r['chunk_number']),\n",
    "    \"word_count\": int(r['word_count'])\n",
    "}, axis=1).tolist()\n",
    "\n",
    "print(\"Documentos a indexar:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7bf78",
   "metadata": {},
   "source": [
    "### 6 ‚Äî Construir √≠ndice ChromaDB y poblarlo con embeddings\n",
    "Usaremos `chromadb` con persistencia local en `data/index/chroma`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e51a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chroma PersistentClient inicializado en: c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\notebooks\\data\\index\\chroma\n",
      "üÜï Colecci√≥n creada: chunks_collection\n",
      "üîç Nuevos docs a insertar: 66 / 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sofia\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79.3M/79.3M [00:03<00:00, 26.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Insertados: 66 (Total progreso: 100/66)\n",
      "\n",
      "üìå Indexaci√≥n completada\n",
      "üìÅ Total en colecci√≥n ahora: 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "# Ruta persistente del √≠ndice\n",
    "project_root = os.getcwd()  # << Mantengo tu l√≥gica\n",
    "CHROMA_PERSIST_DIR = os.path.join(project_root, \"data\", \"index\", \"chroma\")\n",
    "os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "# Inicializar Chroma (modo persistente)\n",
    "client = PersistentClient(path=CHROMA_PERSIST_DIR)\n",
    "print(\"‚úÖ Chroma PersistentClient inicializado en:\", CHROMA_PERSIST_DIR)\n",
    "\n",
    "COLLECTION_NAME = \"chunks_collection\"\n",
    "\n",
    "# Cargar o crear colecci√≥n\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"üìÇ Colecci√≥n existente cargada: {COLLECTION_NAME}\")\n",
    "except Exception:\n",
    "    collection = client.create_collection(name=COLLECTION_NAME)\n",
    "    print(f\"üÜï Colecci√≥n creada: {COLLECTION_NAME}\")\n",
    "\n",
    "# Evitar duplicados\n",
    "try:\n",
    "    existing_docs = collection.get(include=[\"ids\"])\n",
    "    existing_ids = set(existing_docs[\"ids\"])\n",
    "except Exception:\n",
    "    existing_ids = set()\n",
    "\n",
    "to_add_indices = [i for i, _id in enumerate(ids) if _id not in existing_ids]\n",
    "\n",
    "print(f\"üîç Nuevos docs a insertar: {len(to_add_indices)} / {len(ids)}\")\n",
    "\n",
    "# Insertar nuevos chunks en batches\n",
    "BATCH = 100\n",
    "for start in range(0, len(to_add_indices), BATCH):\n",
    "    end = start + BATCH\n",
    "    batch_idx = to_add_indices[start:end]\n",
    "\n",
    "    collection.add(\n",
    "        ids=[ids[i] for i in batch_idx],\n",
    "        documents=[documents[i] for i in batch_idx],\n",
    "        metadatas=[metadatas[i] for i in batch_idx]\n",
    "    )\n",
    "    print(f\"‚úÖ Insertados: {len(batch_idx)} (Total progreso: {end}/{len(to_add_indices)})\")\n",
    "\n",
    "print(\"\\nüìå Indexaci√≥n completada\")\n",
    "print(\"üìÅ Total en colecci√≥n ahora:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2386bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=chunks_collection)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a71f11",
   "metadata": {},
   "source": [
    "### 5 ‚Äî Construir un √≠ndice FAISS local con las mismas embeddings\n",
    "Si `faiss` est√° disponible, construimos un √≠ndice IndexFlatIP (producto interno) sobre embeddings L2-normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1a9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando embeddings para FAISS (todo el conjunto)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:18<00:00,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index creado. N¬∞ vectores: 66\n",
      "FAISS index guardado en: c:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\notebooks\\data\\index\\faiss_index.bin\n"
     ]
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m     df_meta = df.copy()\n\u001b[32m     38\u001b[39m     df_meta[\u001b[33m'\u001b[39m\u001b[33mfaiss_id\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_meta))  \u001b[38;5;66;03m# mapping index position -> metadata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43mdf_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaiss_meta_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMetadata FAISS guardada en:\u001b[39m\u001b[33m\"\u001b[39m, faiss_meta_path)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:174\u001b[39m\n\u001b[32m    167\u001b[39m     pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m         ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m     pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:166\u001b[39m, in \u001b[36mpatch_pyarrow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m         pickletools.dis(serialized, out)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    159\u001b[39m             _ERROR_MSG.format(\n\u001b[32m    160\u001b[39m                 storage_type=storage_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m             )\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow.py_extension_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m     ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pyarrow\\types.pxi:2280\u001b[39m, in \u001b[36mpyarrow.lib.unregister_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sofia\\RAGModel_MineriaMultimedia_202520\\venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "use_faiss = True\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    print(\"faiss no est√° disponible; se omitir√° la construcci√≥n de FAISS.\")\n",
    "    use_faiss = False\n",
    "\n",
    "faiss_index_path = os.path.join(project_root, \"data\", \"index\", \"faiss_index.bin\")\n",
    "faiss_meta_path = os.path.join(project_root, \"data\", \"index\", \"faiss_metadata.parquet\")\n",
    "\n",
    "if use_faiss:\n",
    "    # calcular embeddings para TODO el set\n",
    "    print(\"Calculando embeddings para FAISS (todo el conjunto)...\")\n",
    "    all_embeddings = embed_model.encode(documents, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    # normalizar (IP con vectores unitarios ‚âà cos sim)\n",
    "    def normalize(x):\n",
    "        norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "        norms[norms==0] = 1e-9\n",
    "        return x / norms\n",
    "    all_embeddings = normalize(all_embeddings).astype('float32')\n",
    "\n",
    "    # crear √≠ndice\n",
    "    d = all_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(all_embeddings)\n",
    "    print(\"FAISS index creado. N¬∞ vectores:\", index.ntotal)\n",
    "\n",
    "    # guardar √≠ndice y metadata (ids, metadatas)\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(\"FAISS index guardado en:\", faiss_index_path)\n",
    "\n",
    "    # guardar metadata (ids, formatted id, book_name, etc.)\n",
    "    import pyarrow as pa  # optional, but pandas.to_parquet usa pyarrow\n",
    "    df_meta = df.copy()\n",
    "    df_meta['faiss_id'] = range(len(df_meta))  # mapping index position -> metadata\n",
    "    # Convertir columnas a tipos \"seguros\" para Parquet\n",
    "    df_meta_clean = df_meta.copy()\n",
    "    for col in df_meta_clean.columns:\n",
    "        if df_meta_clean[col].dtype == \"Int64\":     # Int64 nullable ‚Üí int32 o int64 normal\n",
    "            df_meta_clean[col] = df_meta_clean[col].astype(\"int64\")\n",
    "        elif df_meta_clean[col].dtype == \"string\":  # String Arrow type ‚Üí str (object)\n",
    "            df_meta_clean[col] = df_meta_clean[col].astype(str)\n",
    "\n",
    "    df_meta_clean.to_parquet(faiss_meta_path, index=False)\n",
    "    print(\"‚úÖ Metadata FAISS guardada sin Arrow extension types en:\", faiss_meta_path)\n",
    "\n",
    "    print(\"Metadata FAISS guardada en:\", faiss_meta_path)\n",
    "else:\n",
    "    print(\"FAISS no construido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 ‚Äî Funciones de b√∫squeda (Chroma)\n",
    "def chroma_search(query, top_k=5):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)[0].tolist()\n",
    "    res = collection.query(query_embeddings=[q_emb], n_results=top_k, include=['ids','documents','metadatas','distances'])\n",
    "    return res\n",
    "\n",
    "# 7 ‚Äî Funci√≥n de b√∫squeda FAISS (usa embeddings normalizados)\n",
    "if use_faiss:\n",
    "    # recargar √≠ndice y metadata (por si se ejecuta en otra sesi√≥n)\n",
    "    idx = faiss.read_index(faiss_index_path)\n",
    "    import pandas as pd\n",
    "    faiss_meta = pd.read_parquet(faiss_meta_path)\n",
    "\n",
    "    def faiss_search(query, top_k=5):\n",
    "        q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "        # normalizar\n",
    "        q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-9)\n",
    "        D, I = idx.search(q_emb.astype('float32'), top_k)\n",
    "        results = []\n",
    "        for dist, ind in zip(D[0], I[0]):\n",
    "            meta = faiss_meta.iloc[ind].to_dict()\n",
    "            doc = faiss_meta.iloc[ind]['text']\n",
    "            results.append({\"index\": int(ind), \"distance\": float(dist), \"meta\": meta, \"doc\": doc})\n",
    "        return results\n",
    "else:\n",
    "    def faiss_search(query, top_k=5):\n",
    "        raise RuntimeError(\"FAISS no est√° disponible en este entorno.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Who saves Bella from the van?\",\n",
    "    \"Which Cullen family member is a doctor?\",\n",
    "    \"Where does Bella move to?\",\n",
    "    \"What family feeds on animal blood?\"\n",
    "]\n",
    "\n",
    "print(\"=== Chroma results ===\")\n",
    "for q in queries:\n",
    "    print(\"\\n>>> Query:\", q)\n",
    "    r = chroma_search(q, top_k=5)\n",
    "    for i in range(len(r['ids'][0])):\n",
    "        print(f\"Rank {i+1} ‚Äî id: {r['ids'][0][i]} dist={r['distances'][0][i]:.4f}\")\n",
    "        meta = r['metadatas'][0][i]\n",
    "        print(\"  book:\", meta.get('book_name'), \"chunk:\", meta.get('chunk_number'), \"words:\", meta.get('word_count'))\n",
    "        print(\"  snippet:\", r['documents'][0][i][:200].replace('\\n',' '), \"...\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "if use_faiss:\n",
    "    print(\"\\n=== FAISS results ===\")\n",
    "    for q in queries:\n",
    "        print(\"\\n>>> Query:\", q)\n",
    "        res = faiss_search(q, top_k=5)\n",
    "        for i, r in enumerate(res):\n",
    "            print(f\"Rank {i+1} ‚Äî faiss_index: {r['index']} dist={r['distance']:.4f} book={r['meta']['book_name']} chunk={r['meta']['chunk_number']}\")\n",
    "            print(\"  snippet:\", r['doc'][:200].replace('\\n',' '), \"...\")\n",
    "        print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_META_PATH = os.path.join(project_root, \"data\", \"index\", \"chunks_metadata.parquet\")\n",
    "os.makedirs(os.path.dirname(OUT_META_PATH), exist_ok=True)\n",
    "df.to_parquet(OUT_META_PATH, index=False)\n",
    "print(\"Metadata guardada en:\", OUT_META_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_venv)",
   "language": "python",
   "name": "rag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
