import os
import sys
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Any

# Configurar path para importar m√≥dulos
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
src_path = os.path.join(project_root, "src")

if src_path not in sys.path:
    sys.path.insert(0, src_path)

from utils import load_chunks_from_folder


class RAGModelA:
    """
    Modelo RAG A: Embeddings sem√°nticos densos + FAISS.
    
    Pipeline:
    1. Cargar chunks preprocesados
    2. Generar embeddings densos con Sentence Transformers
    3. Indexar con FAISS (b√∫squeda vectorial eficiente)
    4. Realizar consultas sem√°nticas
    
    Este modelo usa b√∫squeda sem√°ntica avanzada, capturando mejor
    el significado de las consultas vs m√©todos l√©xicos como BM25.
    """
    
    def __init__(self, preprocessed_base_dir=None, 
                 model_name="sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
                 use_gpu=False):
        """
        Inicializa el modelo RAG A.
        
        Par√°metros:
        - preprocessed_base_dir: Ruta base a data/preprocessed
        - model_name: Nombre del modelo de embeddings
        - use_gpu: Usar GPU si est√° disponible
        """
        if preprocessed_base_dir is None:
            self.preprocessed_base_dir = os.path.join(project_root, "data", "preprocessed")
        else:
            self.preprocessed_base_dir = preprocessed_base_dir
        
        self.model_name = model_name
        self.use_gpu = use_gpu
        
        self.chunks = []
        self.chunks_metadata = []
        self.embeddings = None
        self.embedding_model = None
        self.faiss_index = None
        self.total_chunks = 0
        self.embedding_dim = None
    
    # ------------------------------------------------------------
    # 1. Cargar chunks preprocesados
    # ------------------------------------------------------------
    def load_preprocessed_chunks(self, chunk_config="processed_400_100"):
        """
        Carga chunks desde las carpetas preprocesadas.
        
        Par√°metros:
        - chunk_config: Nombre de la carpeta (ej: "processed_400_100")
        """
        print(f"\n{'='*80}")
        print(f"CARGANDO CHUNKS PREPROCESADOS")
        print(f"{'='*80}")
        
        folder_path = os.path.join(self.preprocessed_base_dir, chunk_config)
        
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"‚ùå No se encuentra la carpeta: {folder_path}")
        
        print(f"üìÅ Carpeta: {folder_path}")
        
        records = load_chunks_from_folder(folder_path)
        
        if not records:
            raise ValueError(f"No se encontraron chunks en {folder_path}")
        
        df = pd.DataFrame.from_records(records)
        
        self.chunks = df['text'].astype(str).tolist()
        self.chunks_metadata = df.to_dict('records')
        self.total_chunks = len(self.chunks)
        
        print(f"Cargados {self.total_chunks} chunks")
        print(f"Configuraci√≥n: {chunk_config}")
        print(f"{'='*80}\n")
        
        return self.chunks
    
    # ------------------------------------------------------------
    # 2. Cargar modelo de embeddings
    # ------------------------------------------------------------
    def load_embedding_model(self):
        """
        Carga el modelo de Sentence Transformers para generar embeddings.
        """
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError:
            raise ImportError(
                "sentence-transformers no instalado.\n"
                "Instalar con: pip install sentence-transformers"
            )
        
        print(f"\n{'='*80}")
        print(f"CARGANDO MODELO DE EMBEDDINGS")
        print(f"{'='*80}")
        print(f"Modelo: {self.model_name}")
        
        device = 'cuda' if self.use_gpu else 'cpu'
        
        try:
            self.embedding_model = SentenceTransformer(self.model_name, device=device)
            print(f"Modelo cargado en: {device}")
            
            # Obtener dimensi√≥n de embeddings
            test_embedding = self.embedding_model.encode(["test"], convert_to_numpy=True)
            self.embedding_dim = test_embedding.shape[1]
            print(f"Dimensi√≥n embeddings: {self.embedding_dim}")
            
        except Exception as e:
            print(f"Error cargando modelo: {e}")
            print(f"Intentando con modelo alternativo...")
            self.model_name = "all-MiniLM-L6-v2"
            self.embedding_model = SentenceTransformer(self.model_name, device=device)
            test_embedding = self.embedding_model.encode(["test"], convert_to_numpy=True)
            self.embedding_dim = test_embedding.shape[1]
            print(f"Modelo alternativo cargado: {self.model_name}")
        
        print(f"{'='*80}\n")
        
        return self.embedding_model
    
    # ------------------------------------------------------------
    # 3. Generar embeddings densos
    # ------------------------------------------------------------
    def generate_embeddings(self, batch_size=32):
        """
        Genera embeddings densos para todos los chunks.
        
        Par√°metros:
        - batch_size: Tama√±o de batch para procesamiento
        """
        if not self.chunks:
            raise ValueError("No hay chunks cargados.")
        
        if self.embedding_model is None:
            self.load_embedding_model()
        
        print(f"\n{'='*80}")
        print(f"GENERANDO EMBEDDINGS DENSOS")
        print(f"{'='*80}")
        print(f"Total chunks: {len(self.chunks)}")
        print(f"Batch size: {batch_size}")
        
        # Generar embeddings
        self.embeddings = self.embedding_model.encode(
            self.chunks,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # Normalizar para similitud coseno
        )
        
        print(f"Embeddings generados")
        print(f"Shape: {self.embeddings.shape}")
        print(f"Tama√±o en memoria: {self.embeddings.nbytes / 1024 / 1024:.2f} MB")
        print(f"{'='*80}\n")
        
        return self.embeddings
    
    # ------------------------------------------------------------
    # 4. Crear √≠ndice FAISS
    # ------------------------------------------------------------
    def create_faiss_index(self, index_type="flat"):
        """
        Crea √≠ndice FAISS para b√∫squeda vectorial eficiente.
        
        Par√°metros:
        - index_type: Tipo de √≠ndice ('flat', 'ivf', 'hnsw')
            - 'flat': B√∫squeda exacta (m√°s lento, m√°s preciso)
            - 'ivf': Inverted File Index (r√°pido, aproximado)
            - 'hnsw': Hierarchical NSW (muy r√°pido, aproximado)
        """
        if self.embeddings is None:
            raise ValueError("Embeddings no generados. Ejecuta generate_embeddings() primero.")
        
        try:
            import faiss
        except ImportError:
            raise ImportError(
                "faiss no instalado.\n"
                "Instalar con: pip install faiss-cpu  (o faiss-gpu para GPU)"
            )
        
        print(f"\n{'='*80}")
        print(f"CREANDO √çNDICE FAISS")
        print(f"{'='*80}")
        print(f"Tipo de √≠ndice: {index_type.upper()}")
        
        d = self.embedding_dim
        
        if index_type == "flat":
            # IndexFlatIP: Producto interno (equivale a coseno con vectores normalizados)
            self.faiss_index = faiss.IndexFlatIP(d)
            print(f"Usando IndexFlatIP (b√∫squeda exacta)")
        
        elif index_type == "ivf":
            # IVF: M√°s r√°pido pero aproximado
            nlist = min(100, len(self.chunks) // 10)  # N√∫mero de clusters
            quantizer = faiss.IndexFlatIP(d)
            self.faiss_index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)
            
            print(f"Usando IndexIVFFlat (b√∫squeda aproximada)")
            print(f"Clusters: {nlist}")
            print(f"Entrenando √≠ndice...")
            
            # Entrenar IVF
            self.faiss_index.train(self.embeddings.astype('float32'))
            print(f"√çndice entrenado")
        
        elif index_type == "hnsw":
            # HNSW: Muy r√°pido, buena precisi√≥n
            M = 32  # N√∫mero de conexiones por nodo
            self.faiss_index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_INNER_PRODUCT)
            print(f"Usando IndexHNSWFlat (grafo jer√°rquico)")
            print(f"Conexiones por nodo: {M}")
        
        else:
            raise ValueError(f"Tipo de √≠ndice desconocido: {index_type}")
        
        # Agregar embeddings al √≠ndice
        print(f"Agregando {len(self.embeddings)} vectores al √≠ndice...")
        self.faiss_index.add(self.embeddings.astype('float32'))
        
        print(f"√çndice FAISS creado")
        print(f"Total vectores: {self.faiss_index.ntotal}")
        print(f"{'='*80}\n")
        
        return self.faiss_index
    
    # ------------------------------------------------------------
    # 5. Pipeline completo: cargar + embedd + indexar
    # ------------------------------------------------------------
    def prepare_documents(self, chunk_config="processed_400_100", 
                         index_type="flat", batch_size=32):
        """
        Pipeline completo: carga, genera embeddings e indexa.
        
        Par√°metros:
        - chunk_config: Configuraci√≥n de chunks
        - index_type: Tipo de √≠ndice FAISS
        - batch_size: Tama√±o de batch para embeddings
        """
        print(f"\n{'#'*80}")
        print(f"# PIPELINE MODEL A: EMBEDDINGS DENSOS + FAISS")
        print(f"{'#'*80}\n")
        
        # Paso 1: Cargar chunks
        self.load_preprocessed_chunks(chunk_config)
        
        # Paso 2: Cargar modelo de embeddings
        self.load_embedding_model()
        
        # Paso 3: Generar embeddings
        self.generate_embeddings(batch_size=batch_size)
        
        # Paso 4: Crear √≠ndice FAISS
        self.create_faiss_index(index_type=index_type)
        
        print(f"\n{'#'*80}")
        print(f"#PIPELINE COMPLETADO")
        print(f"{'#'*80}")
        print(f"Resumen:")
        print(f"  ‚Ä¢ Total chunks:        {self.total_chunks}")
        print(f"  ‚Ä¢ Modelo embeddings:   {self.model_name}")
        print(f"  ‚Ä¢ Dimensi√≥n:           {self.embedding_dim}")
        print(f"  ‚Ä¢ Tipo √≠ndice FAISS:   {index_type.upper()}")
        print(f"  ‚Ä¢ Vectores indexados:  {self.faiss_index.ntotal}")
        print(f"{'#'*80}\n")
    
    # ------------------------------------------------------------
    # 6. B√∫squeda sem√°ntica
    # ------------------------------------------------------------
    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        B√∫squeda sem√°ntica usando FAISS.
        
        Par√°metros:
        - query: Texto de consulta
        - top_k: N√∫mero de resultados
        
        Retorna:
        - Lista de tuplas (√≠ndice, score)
        """
        if self.faiss_index is None:
            raise ValueError("√çndice no inicializado. Ejecuta prepare_documents() primero.")
        
        # Generar embedding de la consulta
        query_embedding = self.embedding_model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        ).astype('float32')
        
        # Buscar en FAISS
        distances, indices = self.faiss_index.search(query_embedding, top_k)
        
        # Convertir a lista de tuplas
        results = [(int(idx), float(dist)) for idx, dist in zip(indices[0], distances[0])]
        
        return results
    
    # ------------------------------------------------------------
    # 7. Consulta de alto nivel
    # ------------------------------------------------------------
    def query(self, query_text: str, top_k: int = 3, show_details: bool = True) -> List[Dict[str, Any]]:
        """
        Realiza una consulta sem√°ntica con detalles completos.
        
        Par√°metros:
        - query_text: Texto de la consulta
        - top_k: N√∫mero de resultados
        - show_details: Mostrar detalles de resultados
        
        Retorna:
        - Lista de diccionarios con resultados completos
        """
        if show_details:
            print(f"\n{'='*80}")
            print(f"CONSULTA SEM√ÅNTICA: '{query_text}'")
            print(f"{'='*80}\n")
        
        # Buscar
        search_results = self.search(query_text, top_k)
        
        # Formatear resultados
        results = []
        for rank, (idx, score) in enumerate(search_results, 1):
            chunk = self.chunks[idx]
            metadata = self.chunks_metadata[idx]
            
            result = {
                'rank': rank,
                'index': idx,
                'score': score,
                'text': chunk,
                'metadata': metadata
            }
            
            results.append(result)
            
            if show_details:
                print(f"RANK {rank} | Similarity: {score:.4f}")
                print(f"Libro: {metadata.get('book_name', 'N/A')}")
                print(f"Chunk #{metadata.get('chunk_number', 'N/A')}")
                print(f"Palabras: {metadata.get('word_count', 'N/A')}")
                print(f"Texto: {chunk[:300]}...")
                print("-" * 80)
        
        if show_details and results:
            print(f"\n RESPUESTA BASADA EN CONTEXTO M√ÅS RELEVANTE:")
            print(f"{results[0]['text'][:500]}...")
            print("=" * 80 + "\n")
        
        return results
    
    # ------------------------------------------------------------
    # 8. B√∫squeda batch
    # ------------------------------------------------------------
    def batch_query(self, queries: List[str], top_k: int = 3) -> List[List[Dict]]:
        """
        Ejecuta m√∫ltiples consultas de forma eficiente.
        
        Par√°metros:
        - queries: Lista de consultas
        - top_k: N√∫mero de resultados por consulta
        
        Retorna:
        - Lista de listas con resultados
        """
        print(f"\n{'='*80}")
        print(f"EJECUCI√ìN BATCH: {len(queries)} consultas")
        print(f"{'='*80}\n")
        
        all_results = []
        
        for i, query in enumerate(queries, 1):
            print(f"[{i}/{len(queries)}] {query}")
            results = self.query(query, top_k=top_k, show_details=False)
            all_results.append(results)
            
            if results:
                print(f"  ‚Üí Top-1: {results[0]['text'][:100]}... (Score: {results[0]['score']:.3f})")
        
        print(f"\n{'='*80}")
        print(f"Batch completado")
        print(f"{'='*80}\n")
        
        return all_results
    
    # ------------------------------------------------------------
    # 9. Guardar y cargar √≠ndice
    # ------------------------------------------------------------
    def save_index(self, output_dir: str):
        """
        Guarda el √≠ndice FAISS y embeddings en disco.
        
        Par√°metros:
        - output_dir: Directorio de salida
        """
        try:
            import faiss
        except ImportError:
            raise ImportError("faiss no disponible")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Guardar √≠ndice FAISS
        index_path = os.path.join(output_dir, "faiss_index.bin")
        faiss.write_index(self.faiss_index, index_path)
        
        # Guardar embeddings
        embeddings_path = os.path.join(output_dir, "embeddings.npy")
        np.save(embeddings_path, self.embeddings)
        
        # Guardar metadata
        metadata_path = os.path.join(output_dir, "chunks_metadata.parquet")
        df_meta = pd.DataFrame(self.chunks_metadata)
        df_meta['chunk_text'] = self.chunks
        df_meta.to_parquet(metadata_path, index=False)
        
        # Guardar configuraci√≥n
        config = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'total_chunks': self.total_chunks
        }
        
        import json
        config_path = os.path.join(output_dir, "config.json")
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"\n √çndice guardado en: {output_dir}")
        print(f"Archivos:")
        print(f"  ‚Ä¢ {index_path}")
        print(f"  ‚Ä¢ {embeddings_path}")
        print(f"  ‚Ä¢ {metadata_path}")
        print(f"  ‚Ä¢ {config_path}\n")
    
    def load_index(self, input_dir: str):
        """
        Carga √≠ndice FAISS y embeddings desde disco.
        
        Par√°metros:
        - input_dir: Directorio con archivos guardados
        """
        try:
            import faiss
        except ImportError:
            raise ImportError("faiss no disponible")
        
        print(f"\n{'='*80}")
        print(f"CARGANDO √çNDICE DESDE DISCO")
        print(f"{'='*80}")
        print(f"Directorio: {input_dir}")
        
        # Cargar configuraci√≥n
        import json
        config_path = os.path.join(input_dir, "config.json")
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        self.model_name = config['model_name']
        self.embedding_dim = config['embedding_dim']
        self.total_chunks = config['total_chunks']
        
        # Cargar modelo de embeddings
        self.load_embedding_model()
        
        # Cargar √≠ndice FAISS
        index_path = os.path.join(input_dir, "faiss_index.bin")
        self.faiss_index = faiss.read_index(index_path)
        
        # Cargar embeddings
        embeddings_path = os.path.join(input_dir, "embeddings.npy")
        self.embeddings = np.load(embeddings_path)
        
        # Cargar metadata
        metadata_path = os.path.join(input_dir, "chunks_metadata.parquet")
        df_meta = pd.read_parquet(metadata_path)
        self.chunks = df_meta['chunk_text'].tolist()
        self.chunks_metadata = df_meta.drop('chunk_text', axis=1).to_dict('records')
        
        print(f"√çndice cargado")
        print(f"Vectores: {self.faiss_index.ntotal}")
        print(f"Chunks: {len(self.chunks)}")
        print(f"{'='*80}\n")
    
    # ------------------------------------------------------------
    # 10. Estad√≠sticas del modelo
    # ------------------------------------------------------------
    def get_stats(self):
        """
        Retorna estad√≠sticas del modelo.
        """
        if not self.chunks:
            return {"error": "No hay chunks cargados"}
        
        stats = {
            "total_chunks": self.total_chunks,
            "modelo_embeddings": self.model_name,
            "dimension_embeddings": self.embedding_dim,
            "vectores_indexados": self.faiss_index.ntotal if self.faiss_index else 0,
            "tamano_embeddings_mb": self.embeddings.nbytes / 1024 / 1024 if self.embeddings is not None else 0,
            "metodo_busqueda": "FAISS (b√∫squeda vectorial densa)"
        }
        
        print(f"\n{'='*80}")
        print(f"ESTAD√çSTICAS DEL MODELO A")
        print(f"{'='*80}")
        for key, value in stats.items():
            label = key.replace('_', ' ').title()
            if isinstance(value, float):
                print(f"  ‚Ä¢ {label}: {value:.2f}")
            else:
                print(f"  ‚Ä¢ {label}: {value}")
        print(f"{'='*80}\n")
        
        return stats
    
    # ------------------------------------------------------------
    # 11. Visualizaci√≥n de embeddings (opcional)
    # ------------------------------------------------------------
    def visualize_embeddings(self, n_samples=100, method='tsne'):
        """
        Visualiza embeddings en 2D usando t-SNE o UMAP.
        
        Par√°metros:
        - n_samples: N√∫mero de muestras a visualizar
        - method: 'tsne' o 'umap'
        """
        if self.embeddings is None:
            raise ValueError("Embeddings no generados")
        
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError("matplotlib no instalado")
        
        # Muestrear embeddings
        indices = np.random.choice(len(self.embeddings), min(n_samples, len(self.embeddings)), replace=False)
        sample_embeddings = self.embeddings[indices]
        
        # Reducir dimensionalidad
        if method == 'tsne':
            from sklearn.manifold import TSNE
            reducer = TSNE(n_components=2, random_state=42)
        elif method == 'umap':
            try:
                import umap
                reducer = umap.UMAP(n_components=2, random_state=42)
            except ImportError:
                raise ImportError("umap no instalado. Instalar con: pip install umap-learn")
        else:
            raise ValueError(f"M√©todo desconocido: {method}")
        
        print(f"Reduciendo dimensionalidad con {method.upper()}...")
        embeddings_2d = reducer.fit_transform(sample_embeddings)
        
        # Visualizar
        plt.figure(figsize=(10, 8))
        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)
        plt.title(f'Visualizaci√≥n de Embeddings ({method.upper()})')
        plt.xlabel('Dimensi√≥n 1')
        plt.ylabel('Dimensi√≥n 2')
        plt.grid(True, alpha=0.3)
        
        output_path = os.path.join(project_root, 'data', f'embeddings_viz_{method}.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"‚úÖ Visualizaci√≥n guardada en: {output_path}")
        plt.show()


# ============================================================
# EJEMPLO DE USO
# ============================================================

if __name__ == "__main__":
    preprocessed_base = os.path.join(project_root, "data", "preprocessed")
    
    print(f"\n{'#'*80}")
    print(f"# MODELO RAG A - EMBEDDINGS DENSOS + FAISS")
    print(f"{'#'*80}\n")
    
    # Inicializar modelo
    rag_a = RAGModelA(
        preprocessed_base_dir=preprocessed_base,
        model_name="sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
        use_gpu=False
    )
    
    # Ejecutar pipeline completo
    rag_a.prepare_documents(
        chunk_config="processed_400_100",
        index_type="flat",  # 'flat', 'ivf', o 'hnsw'
        batch_size=32
    )
    
    # Consultas de ejemplo
    queries = [
        "Who saves Bella from the van?",
        "Which Cullen family member is a doctor?",
        "Where does Bella move to?",
    ]
    
    print(f"\n{'#'*80}")
    print(f"# EJECUTANDO CONSULTAS DE PRUEBA")
    print(f"{'#'*80}\n")
    
    for query in queries:
        results = rag_a.query(query, top_k=3)
        input("\nPresiona Enter para continuar a la siguiente consulta...")
    
    # Mostrar estad√≠sticas
    rag_a.get_stats()
    
    # Guardar √≠ndice
    print("\n¬øDeseas guardar el √≠ndice? (s/n): ", end="")
    respuesta = input().lower()
    if respuesta == 's':
        output_dir = os.path.join(project_root, "data", "index", "model_a")
        rag_a.save_index(output_dir)
    
    # Visualizar embeddings
    print("\n¬øDeseas visualizar embeddings? (s/n): ", end="")
    respuesta = input().lower()
    if respuesta == 's':
        try:
            rag_a.visualize_embeddings(n_samples=50, method='tsne')
        except Exception as e:
            print(f"Error en visualizaci√≥n: {e}")

# Inicializar
rag_a = RAGModelA(use_gpu=False)

# Preparar (una sola vez)
rag_a.prepare_documents("processed_400_100", index_type="flat")

# Guardar para reutilizar
rag_a.save_index("data/index/model_a")

# Cargar en futuras ejecuciones
rag_a.load_index("data/index/model_a")

# Consultar
results = rag_a.query("Who is Bella?", top_k=5)

# Batch
batch_results = rag_a.batch_query(queries, top_k=3)